{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "Using Torch and PyTorch, prepare a single Jupyter Notebook which shows the performance of different optimizers on different neural networks for regression and classification. In your Jupyter Notebook, you may use different number of neural networks with different topology (different number of layers and neurons , different activation functions, different loss functions, different optimizers, different training modes (batch, mini-batch, stochastic)."
      ],
      "metadata": {
        "id": "_Y5YBBTc92I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this notebook, I will be using Torch and Pytorch to show the performance of different optimizers on different neural networks for regression and classification.**"
      ],
      "metadata": {
        "id": "bYp8M6BC-CVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going directly into the code, lets understand what are the terms optimizer, regression and classification."
      ],
      "metadata": {
        "id": "Zh3lompFAk2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer:** In machine learning, optimization algorithms or optimizers are used to find the best values of the parameters of a model in order to minimize the loss function. Optimizers help in adjusting the weights and biases of a neural network during the training process, such that the network can learn to predict the target variable accurately.\n",
        "\n",
        "**Classification:** Classification is a type of machine learning task where the goal is to predict which category or class a given input belongs to. In classification tasks, the input is usually a set of features and the output is a discrete label. For example, a classification algorithm may be used to classify emails as spam or not spam, or to classify images of animals into different species.\n",
        "\n",
        "**Regression:** Regression is a type of machine learning task where the goal is to predict a continuous numerical value based on one or more input features. Regression algorithms are used for tasks such as predicting the price of a house based on its features, or predicting the age of a person based on their demographic information.\n"
      ],
      "metadata": {
        "id": "Gow4Zo9ayxFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I will use MNIST dataset for classification and California Housing dataset for regression.**\n"
      ],
      "metadata": {
        "id": "LWmzrd-HZT7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "Zyxpo9kwudXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we need to import the necessary libraries."
      ],
      "metadata": {
        "id": "DJ0FbEyH-yEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CWEaqUe2shsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seting device to GPU(CUDA) if available, otherwise use CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "qH_oNjXfT0_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device.type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs0rCwRjT45M",
        "outputId": "0822c4a2-995b-45d6-ce04-80e30f4c4d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the transform attribute.\n",
        "T = transforms.Compose([ # Combining multiple image transformations\n",
        "    transforms.ToTensor(), # Converting images to tensors.\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize the tensors.\n",
        "])"
      ],
      "metadata": {
        "id": "o1G0CjnpDfeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uploading the MNIST dataset and splitting the dataset into train, test and validation sets.\n",
        "train_set = torchvision.datasets.MNIST('./data', train=True, transform=T, download=True)\n",
        "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=T, download=True)\n",
        "test_set = torchvision.datasets.MNIST('./data', train=False, transform=T, download=True)"
      ],
      "metadata": {
        "id": "50PE1GRhD1f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating data loaders.\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "O-w-3Qg1RIqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating different models to measure the performance of the optimizers.\n",
        "\n",
        "# This model is a very simple model that only has one linear layer and a softmax activation function.\n",
        "class MNISTModel1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel1, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 10),\n",
        "            nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, self_updated):\n",
        "        return self.model(self_updated)\n",
        "\n",
        "# This model is a slightly more complex, it has two linear layers with a sigmoid activation function between the layers.\n",
        "# The output is passed through softmax activation function.\n",
        "class MNISTModel2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc2 = nn.Linear(16, 10)\n",
        "        self.fc1 = nn.Linear(28*28, 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        out = self.fc2(x)\n",
        "        return torch.softmax(out, dim=1)\n",
        "\n",
        "# This model is similar to previous model. It has additional two linear layers with ReLU activation function between the layers.\n",
        "# # The output is passed through softmax activation function.\n",
        "class MNISTModel3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel3, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Creating a list of models.\n",
        "models = [MNISTModel1(),\n",
        "    MNISTModel2(),\n",
        "    MNISTModel3()]\n",
        "# My goal was to use the models like this, but like I explained further in the notebook, I noticed it did not work correctly.\n",
        "# So I ended up running them seperately."
      ],
      "metadata": {
        "id": "Gv48u217D6r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of optimizers.\n",
        "optimizers = [[torch.optim.SGD(models[i].parameters(), lr=0.1, momentum=0.9),\n",
        "        torch.optim.Adam(models[i].parameters(), lr=0.01),\n",
        "        torch.optim.RMSprop(models[i].parameters(), lr=0.01, alpha=0.9)\n",
        "    ]\n",
        "    for i in range(len(models))] #This for loop iterates over a range of values from 0 to the length of the models list minus 1, assigning each value to the variable i."
      ],
      "metadata": {
        "id": "_ujwWtILfIyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of loss functions to use during training.\n",
        "loss_functions = [nn.CrossEntropyLoss(), nn.MultiMarginLoss()]"
      ],
      "metadata": {
        "id": "JR6lcQ-ue10H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list with different batch sizes. 1, 64 and len(X_train) are for stochastic, mini-batch and batch modes, respectively.\n",
        "batch_sizes = [1, 64, len(train_loader)]"
      ],
      "metadata": {
        "id": "Xb2pQw2SVMf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "\n",
        "# To train our model, we are going to create a nested loop.\n",
        "# We will try different combinations of optimizers, loss functions and batch modes on each different model we have created.\n",
        "\n",
        "# We will use this part to find the best combination at the end of our training.\n",
        "best_accuracy = 0.0\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "best_loss_fn = None\n",
        "best_batch_size = None\n",
        "\n",
        "# Setting the number of epochs.\n",
        "epochs_number = 2      # I used 2 because it took so much time to run and the system stopped :((\n",
        "\n",
        "# Creating a nested loop to try different combinations.\n",
        "for model in models:\n",
        "  model_name = model.__class__.__name__\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  # Creating empty lists.\n",
        "  train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
        "\n",
        "  for optimizer in optimizers[0]: # I used optimizers[0] because we have two elements in optimizers list. We will use the first one to get the optimizers.\n",
        "      optimizer_name = optimizer.__class__.__name__\n",
        "\n",
        "      for loss_fn in loss_functions:\n",
        "        lossfn_name = loss_fn.__class__.__name__\n",
        "\n",
        "        for batch_size in batch_sizes:\n",
        "\n",
        "          # Printing the combination before every combination.\n",
        "          print(\"Training on\", model_name, \"using\", optimizer_name, \"as the optimizer and\", lossfn_name, \"as the loss function.\", \"Batch mode:\", batch_size)\n",
        "\n",
        "          # Creating a loop to go over the training \"epochs\" times we set.\n",
        "          for e in range(epochs_number):\n",
        "              # Initializing the running loss to zero for the current epoch.\n",
        "              running_loss = 0.0\n",
        "              # Initializing the running number of correct predictions to zero for the current epoch.\n",
        "              running_corrects = 0\n",
        "              # Setting the model to train mode which turns on the gradient computation and updates the model's parameters during backpropagation.\n",
        "              model.train()\n",
        "\n",
        "              # Looping over the training set and loading a batch of input images and their corresponding target labels.\n",
        "              for inputs, targets in train_loader:\n",
        "                  # This clears the gradients of all optimized variables to zero before the forward pass.\n",
        "                  optimizer.zero_grad()\n",
        "                  # This computes the forward pass of the model on the input batch.\n",
        "                  outputs = model(inputs)\n",
        "                  #  This computes the loss between the model's predictions and the target labels.\n",
        "                  loss = loss_fn(outputs, targets)\n",
        "                  # This computes the gradients of the loss with respect to the model parameters.\n",
        "                  loss.backward()\n",
        "                  # This updates the model parameters by taking a step in the direction of the negative gradient using the optimizer.\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # This gets the index of the maximum log-probability predicted by the model for each input image in the batch.\n",
        "                  _, preds = torch.max(outputs, 1)\n",
        "                  # This adds the product of the loss and the batch size to the running loss for the current epoch.\n",
        "                  running_loss += loss.item() * inputs.size(0)\n",
        "                  #  This adds the number of correct predictions to the running number of correct predictions for the current epoch.\n",
        "                  running_corrects += torch.sum(preds == targets.data)\n",
        "\n",
        "              # This calculates the average loss for the current epoch.\n",
        "              epoch_loss = running_loss / len(train_loader.dataset)\n",
        "              # This calculates the accuracy for the current epoch.\n",
        "              epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "              # This appends the average loss for the current epoch to a list of training losses for all epochs.\n",
        "              train_losses.append(epoch_loss)\n",
        "              # This appends the accuracy for the current epoch to a list of training accuracies for all epochs.\n",
        "              train_accuracies.append(epoch_acc)\n",
        "\n",
        "              # This initializes the running loss to zero for the current epoch.\n",
        "              running_loss = 0.0\n",
        "              # This initializes the running number of correct predictions to zero for the current epoch.\n",
        "              running_corrects = 0\n",
        "              # This sets the model to evaluation mode which turns off the gradient computation and freezes the model's parameters.\n",
        "              model.eval()\n",
        "              # This temporarily disables gradient computation to speed up model inference and conserve memory.\n",
        "              with torch.no_grad():\n",
        "                  # This loops over the validation set and loads a batch of input images and their corresponding target labels.\n",
        "                  for inputs, targets in validation_loader:\n",
        "                      # This computes the forward pass of the model on the input batch.\n",
        "                      outputs = model(inputs)\n",
        "                      # This computes the loss between the model's predictions and the target labels.\n",
        "                      loss = loss_fn(outputs, targets)\n",
        "\n",
        "                      # This gets the index of the maximum log-probability predicted by the model for each input image in the batch.\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      # This adds the product of the loss and the batch size to the running loss for the current epoch.\n",
        "                      running_loss += loss.item() * inputs.size(0)\n",
        "                      # This adds the number of correct predictions to the running number of correct predictions.\n",
        "                      running_corrects += torch.sum(preds == targets.data)\n",
        "\n",
        "                  # Compute the average loss per image in the validation set for the current epoch.\n",
        "                  epoch_loss = running_loss / len(validation_loader.dataset)\n",
        "                  # Compute the accuracy for the current epoch by dividing the number of correctly classified images by the total number of images in the validation set.\n",
        "                  epoch_acc = running_corrects.double() / len(validation_loader.dataset)\n",
        "                  # Append the current epoch's loss to the list of losses for all epochs.\n",
        "                  val_losses.append(epoch_loss)\n",
        "                  # Append the current epoch's accuracy to the list of accuracies for all epochs.\n",
        "                  val_accuracies.append(epoch_acc)\n",
        "              # Print the current epoch number, training loss, training accuracy, validation loss, and validation accuracy in a formatted string.\n",
        "              print(f\"Epoch {e+1}/{epochs_number}, Validation Loss: {epoch_loss:.4f}, Validation Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "          # Checking if the current model's accuracy(last line of val_accuricies list) is better than the previous best accuracy\n",
        "          if val_accuracies[-1] > best_accuracy:\n",
        "              best_accuracy = val_accuracies[-1]\n",
        "              best_model = model_name\n",
        "              best_optimizer = optimizer_name\n",
        "              best_loss_fn = lossfn_name\n",
        "              best_batch_size = batch_size"
      ],
      "metadata": {
        "id": "Oiua1WRzVcIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf3aa32-5440-4e6f-a059-c62b1777c3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on MNISTModel1 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 1.8055, Validation Accuracy: 0.6547\n",
            "Epoch 2/2, Validation Loss: 1.8054, Validation Accuracy: 0.6538\n",
            "Training on MNISTModel1 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 1.7898, Validation Accuracy: 0.6695\n",
            "Epoch 2/2, Validation Loss: 1.7883, Validation Accuracy: 0.6714\n",
            "Training on MNISTModel1 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 1.7883, Validation Accuracy: 0.6715\n",
            "Epoch 2/2, Validation Loss: 1.7900, Validation Accuracy: 0.6701\n",
            "Training on MNISTModel1 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 0.3306, Validation Accuracy: 0.6715\n",
            "Epoch 2/2, Validation Loss: 0.3307, Validation Accuracy: 0.6714\n",
            "Training on MNISTModel1 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 0.3282, Validation Accuracy: 0.6736\n",
            "Epoch 2/2, Validation Loss: 0.3279, Validation Accuracy: 0.6736\n",
            "Training on MNISTModel1 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 0.3322, Validation Accuracy: 0.6689\n",
            "Epoch 2/2, Validation Loss: 0.3264, Validation Accuracy: 0.6755\n",
            "Training on MNISTModel1 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 1.7964, Validation Accuracy: 0.6639\n",
            "Epoch 2/2, Validation Loss: 1.7939, Validation Accuracy: 0.6659\n",
            "Training on MNISTModel1 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 1.8022, Validation Accuracy: 0.6586\n",
            "Epoch 2/2, Validation Loss: 1.7983, Validation Accuracy: 0.6619\n",
            "Training on MNISTModel1 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 1.7914, Validation Accuracy: 0.6688\n",
            "Epoch 2/2, Validation Loss: 1.7919, Validation Accuracy: 0.6688\n",
            "Training on MNISTModel1 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 0.3321, Validation Accuracy: 0.6679\n",
            "Epoch 2/2, Validation Loss: 0.3297, Validation Accuracy: 0.6702\n",
            "Training on MNISTModel1 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 0.3368, Validation Accuracy: 0.6631\n",
            "Epoch 2/2, Validation Loss: 0.3401, Validation Accuracy: 0.6599\n",
            "Training on MNISTModel1 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 0.3332, Validation Accuracy: 0.6667\n",
            "Epoch 2/2, Validation Loss: 0.3485, Validation Accuracy: 0.6517\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 1.7882, Validation Accuracy: 0.6729\n",
            "Epoch 2/2, Validation Loss: 1.7875, Validation Accuracy: 0.6732\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 1.7902, Validation Accuracy: 0.6704\n",
            "Epoch 2/2, Validation Loss: 1.7993, Validation Accuracy: 0.6611\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 1.7888, Validation Accuracy: 0.6715\n",
            "Epoch 2/2, Validation Loss: 1.7907, Validation Accuracy: 0.6697\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
            "Epoch 1/2, Validation Loss: 0.3298, Validation Accuracy: 0.6702\n",
            "Epoch 2/2, Validation Loss: 0.3287, Validation Accuracy: 0.6717\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
            "Epoch 1/2, Validation Loss: 0.3473, Validation Accuracy: 0.6528\n",
            "Epoch 2/2, Validation Loss: 0.3415, Validation Accuracy: 0.6587\n",
            "Training on MNISTModel1 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
            "Epoch 1/2, Validation Loss: 0.3342, Validation Accuracy: 0.6654\n",
            "Epoch 2/2, Validation Loss: 0.3261, Validation Accuracy: 0.6741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The best combination is: Model: {best_model}, Optimizer: {best_optimizer}, Loss Function: {best_loss_fn}, Batch Size: {best_batch_size}, with Validation Accuracy: {best_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPi3ETEwW-LV",
        "outputId": "342bd644-69a7-4e6b-b77f-3a4d5bae17e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best combination is: Model: MNISTModel1, Optimizer: SGD, Loss Function: MultiMarginLoss, Batch Size: 938, with Validation Accuracy: 0.6755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When I run the training model for all three models together, I noticed a bug in my code. The code worked without a problem for the first model, but when it moved to other model, the accuracy dropped around to 0.05-0.15.\n",
        "\n",
        "That is why I run the code for each model seperately and added the results belove."
      ],
      "metadata": {
        "id": "yfKfDQOqIG2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output for second model:**\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.6620, Validation Accuracy: 0.8356\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.6238, Validation Accuracy: 0.8482\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5540, Validation Accuracy: 0.9218\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5529, Validation Accuracy: 0.9198\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5469, Validation Accuracy: 0.9243\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5443, Validation Accuracy: 0.9259\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0826, Validation Accuracy: 0.9263\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0800, Validation Accuracy: 0.9282\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0857, Validation Accuracy: 0.9235\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0759, Validation Accuracy: 0.9343\n",
        "\n",
        "Training on MNISTModel2 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0829, Validation Accuracy: 0.9253\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0735, Validation Accuracy: 0.9329\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5732, Validation Accuracy: 0.8929\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5516, Validation Accuracy: 0.9111\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5633, Validation Accuracy: 0.9004\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5625, Validation Accuracy: 0.9045\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5594, Validation Accuracy: 0.9043\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5526, Validation Accuracy: 0.9102\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.1070, Validation Accuracy: 0.8979\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0891, Validation Accuracy: 0.9144\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.1078, Validation Accuracy: 0.8988\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.1089, Validation Accuracy: 0.8956\n",
        "\n",
        "Training on MNISTModel2 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.1143, Validation Accuracy: 0.8894\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.1053, Validation Accuracy: 0.8975\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5494, Validation Accuracy: 0.9136\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5513, Validation Accuracy: 0.9113\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5461, Validation Accuracy: 0.9151\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5399, Validation Accuracy: 0.9221\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.5407, Validation Accuracy: 0.9220\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.5371, Validation Accuracy: 0.9247\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0830, Validation Accuracy: 0.9185\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0745, Validation Accuracy: 0.9265\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0784, Validation Accuracy: 0.9226\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0776, Validation Accuracy: 0.9236\n",
        "\n",
        "Training on MNISTModel2 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.0781, Validation Accuracy: 0.9229\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.0742, Validation Accuracy: 0.9271\n",
        "\n",
        "\n",
        "**The best combination is: Model: MNISTModel2, Optimizer: SGD, Loss Function: MultiMarginLoss, Batch Size: 64, with Validation Accuracy: 0.9343**\n"
      ],
      "metadata": {
        "id": "KyGZdIK2fAKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output for third model:**\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.7895, Validation Accuracy: 0.6700\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.7852, Validation Accuracy: 0.6748\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.7869, Validation Accuracy: 0.6737\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.7812, Validation Accuracy: 0.6792\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.8000, Validation Accuracy: 0.6606\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.8317, Validation Accuracy: 0.6291\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.3725, Validation Accuracy: 0.6274\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.3226, Validation Accuracy: 0.6773\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.3470, Validation Accuracy: 0.6530\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.3748, Validation Accuracy: 0.6251\n",
        "\n",
        "Training on MNISTModel3 using SGD as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.3412, Validation Accuracy: 0.6589\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.3861, Validation Accuracy: 0.6139\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.6982, Validation Accuracy: 0.7622\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.6962, Validation Accuracy: 0.7647\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.7651, Validation Accuracy: 0.6961\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.6898, Validation Accuracy: 0.7713\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 1.7111, Validation Accuracy: 0.7497\n",
        "\n",
        "Epoch 2/2, Validation Loss: 1.6244, Validation Accuracy: 0.8366\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.1906, Validation Accuracy: 0.8097\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.1456, Validation Accuracy: 0.8543\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.1632, Validation Accuracy: 0.8369\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.1610, Validation Accuracy: 0.8391\n",
        "\n",
        "Training on MNISTModel3 using Adam as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.2258, Validation Accuracy: 0.7743\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.1929, Validation Accuracy: 0.8070\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and CrossEntropyLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 2.3632, Validation Accuracy: 0.0980\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 1\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 64\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "Training on MNISTModel3 using RMSprop as the optimizer and MultiMarginLoss as the loss function. Batch mode: 938\n",
        "\n",
        "Epoch 1/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "Epoch 2/2, Validation Loss: 0.9020, Validation Accuracy: 0.0980\n",
        "\n",
        "**The best combination is: Model: MNISTModel3, Optimizer: Adam, Loss Function: MultiMarginLoss, Batch Size: 1, with Validation Accuracy: 0.8543**"
      ],
      "metadata": {
        "id": "HFCE8UHffAS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, we can say that the best combination was:\n",
        "\n",
        "**Model: MNISTModel2, Optimizer: SGD, Loss Function: MultiMarginLoss, Batch Size: 64, with Validation Accuracy: 0.9343**"
      ],
      "metadata": {
        "id": "I5NSa-Rizso7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "Apyx5Ym4dYzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For regression I will use \"California Housing Dataset\".\n",
        "This dataset contains information about the median house values for various districts in California.\n",
        "\n",
        "The goal of this dataset is to use the available features to predict the median house value in the corresponding district."
      ],
      "metadata": {
        "id": "gmZI5jRKdd7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the California Housing dataset using sklearn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "quPyB0vqqazV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the California Housing dataset\n",
        "dataset = fetch_california_housing()\n",
        "\n",
        "# Splitting the dataset into training and testing sets using train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float().view(-1, 1)\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float().view(-1, 1)"
      ],
      "metadata": {
        "id": "BHYg7Yf4K1MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the torch.nn.functional that we are going to use while creating our models.\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Vp6pWM7ukTsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the different models\n",
        "\n",
        "#First model has three layers with 16, 8, and output_dim number of neurons in each layer, respectively.\n",
        "#It uses ReLU activation function for the first two layers and sigmoid activation function for the last layer.\n",
        "class RegressionModel1(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 16)\n",
        "        self.layer2 = nn.Linear(16, 8)\n",
        "        self.layer3 = nn.Linear(8, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "#The second model is similar to the first model, but with more neurons in each layer.\n",
        "#It has three layers with 32, 16, and output_dim number of neurons in each layer, respectively.\n",
        "#It also uses ReLU activation function for the first two layers and sigmoid activation function for the last layer.\n",
        "class RegressionModel2(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.layer2 = nn.Linear(32, 16)\n",
        "        self.layer3 = nn.Linear(16, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "#The third model is the most complex of the three. This model has four layers.\n",
        "#It has 64, 32, 16, and output_dim number of neurons in each layer, respectively.\n",
        "#It also uses ReLU activation function for the first three layers and sigmoid activation function for the last layer.\n",
        "class RegressionModel3(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(16, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.relu(self.layer3(x))\n",
        "        x = self.sigmoid(self.layer4(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-MBT8M0V-8CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the dimensions of the input and output data.\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "#I created a list named \"models\" to use three models with different optimizers and loss functions during model training.\n",
        "models = [\n",
        "    RegressionModel1(input_dim, output_dim),\n",
        "    RegressionModel2(input_dim, output_dim),\n",
        "    RegressionModel3(input_dim, output_dim)]"
      ],
      "metadata": {
        "id": "nCKUe6H8BVCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of suitable loss functions to use during training.\n",
        "loss_functions = [nn.MSELoss(), nn.L1Loss(), nn.SmoothL1Loss()]"
      ],
      "metadata": {
        "id": "QNthxEMw_cre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of optimizers.\n",
        "optimizers = [[torch.optim.SGD(models[i].parameters(), lr=0.1, momentum=0.9),\n",
        "        torch.optim.Adam(models[i].parameters(), lr=0.01),\n",
        "        torch.optim.RMSprop(models[i].parameters(), lr=0.01, alpha=0.9),\n",
        "        torch.optim.Adagrad(models[i].parameters(), lr=0.1),\n",
        "        torch.optim.Adadelta(models[i].parameters(), lr=0.1)\n",
        "    ]\n",
        "    for i in range(len(models))] #This for loop iterates over a range of values from 0 to the length of the models list minus 1, assigning each value to the variable i."
      ],
      "metadata": {
        "id": "v-qFkdOQ_fdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list with different batch sizes. 1, 64 and len(X_train) are for stochastic, mini-batch and batch modes, respectively.\n",
        "batch_sizes = [1, 64, len(X_train)]"
      ],
      "metadata": {
        "id": "geWLaGhOD9Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Training the model\n",
        "\n",
        "# To train our model, we are going to create a nested loop.\n",
        "# We will try different combinations of optimizers, loss functions and batch modes on each different model we have created.\n",
        "\n",
        "# We will use this part to find the best combination at the end of our training.\n",
        "best_loss = float('inf')\n",
        "best_model = None\n",
        "best_optimizer = None\n",
        "best_loss_fn = None\n",
        "best_batch_size = None\n",
        "\n",
        "# Setting the number of epochs.\n",
        "epochs = 10\n",
        "\n",
        "# Creating a nested loop to try different combinations.\n",
        "for model in models:\n",
        "    model_name = model.__class__.__name__\n",
        "    for optimizer in optimizers[0]:\n",
        "        optimizer_name = optimizer.__class__.__name__\n",
        "        for loss_fn in loss_functions:\n",
        "            lossfn_name = loss_fn.__class__.__name__\n",
        "            for batch_size in batch_sizes:\n",
        "                #batchsize_name = batch_size.__class__.__name__\n",
        "                # Printing the combination before every combination.\n",
        "                print(\"Training on\", model_name, \"using\", optimizer_name, \"as the optimizer and\", lossfn_name, \"as the loss function.\", \"Batch mode:\", batch_size)\n",
        "\n",
        "                # Creating a loop to go over the training \"epochs\" times we set.\n",
        "                for epoch in range(epochs):\n",
        "                    # Training our model\n",
        "                    for i, (inputs, labels) in enumerate(train_loader):\n",
        "                        # Creating the gradients of all optimized tensors before computing the gradients for the current batch.\n",
        "                        optimizer.zero_grad()\n",
        "                        # Passing the input data through the model and generating predictions for the current batch.\n",
        "                        outputs = model(inputs)\n",
        "                        # Computing the loss between the predicted outputs and the actual output labels for the current batch, using the specified loss function.\n",
        "                        loss = loss_fn(outputs, labels)\n",
        "                        # Computing the gradients of the loss with respect to the parameters of the model.\n",
        "                        loss.backward()\n",
        "                        # Updating the parameters of the model using the computed gradients, based on the optimizer's update rule.\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # Printing the training loss for every 10 epochs.\n",
        "                    if epoch % 10 == 9:\n",
        "                        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Evaluating the model on the test set.\n",
        "                # The torch.no_grad() block is used to prevent the computation of gradients during the evaluation, as they are not needed and would slow down the computation.\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(X_test)\n",
        "                    test_loss = loss_fn(outputs, y_test)\n",
        "                    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "                # Checking if this is the best model so far\n",
        "                if test_loss.item() < best_loss:\n",
        "                    best_loss = test_loss.item()\n",
        "                    best_model = model_name\n",
        "                    best_optimizer = optimizer_name\n",
        "                    best_loss_fn = lossfn_name\n",
        "                    best_batch_size = batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIYwe57TF189",
        "outputId": "5803bd38-4980-4c64-b777-e690c235c15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on RegressionModel1 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.8501\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.5862\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 2.2678\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.0683\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.4226\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.2944\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.6973\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.8041\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.9862\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 2.2038\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.1395\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.6983\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9742\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.4746\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.3750\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9649\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.8185\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.6983\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 2.5801\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.1327\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 2.1658\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9595\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.2698\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.0114\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.7321\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.7072\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.8703\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.5384\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.7796\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.9748\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9934\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.0846\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.0706\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.5627\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.6051\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.7102\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 2.7470\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.6107\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 3.2787\n",
            "Test Loss: 2.4234\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.3286\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.1416\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.3000\n",
            "Test Loss: 1.1408\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.0139\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.8906\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel1 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.9914\n",
            "Test Loss: 0.7584\n",
            "Training on RegressionModel2 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 4.8080\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.4907\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.5554\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.5518\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.5028\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.5421\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9723\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.0783\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.2867\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.2737\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 4.2829\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.0075\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.4449\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.5712\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.7047\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.0279\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.1606\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 0.9585\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.8111\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 2.8348\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.0351\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.6872\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.3985\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.5160\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.0729\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.1350\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.3878\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 4.1314\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.7633\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 3.4472\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.5246\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.3959\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.6291\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.2089\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.2164\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.1312\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.9152\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.2921\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 3.3168\n",
            "Test Loss: 3.7841\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.5818\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.6915\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.6041\n",
            "Test Loss: 1.5733\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.3489\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 0.9832\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel2 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.1932\n",
            "Test Loss: 1.1285\n",
            "Training on RegressionModel3 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 4.4909\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.9734\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using SGD as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 5.0971\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.4198\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.8461\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using SGD as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.4454\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9435\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.2477\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using SGD as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.0583\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.6846\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 4.2528\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adam as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.9411\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.5854\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.3055\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adam as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.9233\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 0.9061\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.0177\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adam as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.0621\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 3.8151\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.3220\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.4941\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.5139\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.7430\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.5288\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.2265\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.2458\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using RMSprop as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.3211\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 4.3010\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.4722\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 5.3848\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.7028\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.7373\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.5378\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.1935\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.1889\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adagrad as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.2113\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 4.2430\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 3.2125\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and MSELoss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 4.4551\n",
            "Test Loss: 3.8851\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.7679\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.7516\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and L1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.5050\n",
            "Test Loss: 1.6044\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 1\n",
            "Epoch [10/10], Loss: 1.2976\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 64\n",
            "Epoch [10/10], Loss: 1.4817\n",
            "Test Loss: 1.1545\n",
            "Training on RegressionModel3 using Adadelta as the optimizer and SmoothL1Loss as the loss function. Batch mode: 16512\n",
            "Epoch [10/10], Loss: 1.0060\n",
            "Test Loss: 1.1545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the best combination\n",
        "print(\"Best combination:\")\n",
        "print(f\"Model: {best_model}\")\n",
        "print(f\"Optimizer: {best_optimizer}\")\n",
        "print(f\"Loss function: {best_loss_fn}\")\n",
        "print(f\"Batch size: {best_batch_size}\")\n",
        "print(f\"Test loss: {best_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAhRwcyPract",
        "outputId": "f2a314a1-b89c-4a4f-f47b-75b7805a060c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best combination:\n",
            "Model: RegressionModel1\n",
            "Optimizer: Adam\n",
            "Loss function: SmoothL1Loss\n",
            "Batch size: 1\n",
            "Test loss: 0.7584\n"
          ]
        }
      ]
    }
  ]
}